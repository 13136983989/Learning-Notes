{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "\t\t\t\t\t\t\t\t\t\t\t#x=np.array([[1001,1002],[3,4]])\n",
    "\tif len(x.shape) > 1:\t\t\t\t\t#x.shape=(2, 2)  len(x.shape)=2\t\t\t\t\n",
    "\t\ttmp = np.max(x, axis = 1)\t\t\t#np.max(x, axis = 1)=array([1002,  4])， max in each row\n",
    "\t\tx -= tmp.reshape((x.shape[0], 1))\t#tmp.reshape((x.shape[0], 1))， tmp becomes 2row1column\n",
    "\t\tx = np.exp(x)\t\t\t\t\t\t#xi - max this row, then exp\n",
    "\t\ttmp = np.sum(x, axis = 1)\t\t\t#array([ 1.36787944,  1.36787944])，sum of each row\n",
    "\t\tx /= tmp.reshape((x.shape[0], 1))\t#xi / sum this row\n",
    "\t\n",
    "\telse:\t\t\t\t\t\t\t\t\t#x=[1,2]   x.shape=(2,)   len(x.shape)=1\n",
    "\t\ttmp = np.max(x)\n",
    "\t\tx -= tmp\n",
    "\t\tx = np.exp(x)\n",
    "\t\ttmp = np.sum(x)\n",
    "\t\tx /= tmp\n",
    "\t\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function: \n",
    "# for each element in x\n",
    "# compare derivative calculated by formular and calculus\n",
    "# f: 1st parameter is cost function, 2nd parameter is gradient\n",
    "def gradcheck_naive(f, x):\n",
    "\t\n",
    "\t#Return an object capturing the current internal state of the generator\n",
    "\trndstate = random.getstate()\t\t\t#why use state??????\n",
    "\trandom.setstate(rndstate)\n",
    "\tfx, grad = f(x)\t\t\t\t\t\t\t#fx=np.sum(x ** 2), grad=x * 2 \n",
    "\th = 1e-4\n",
    "\t\n",
    "\t#Efficient multi-dimensional iterator object to iterate over arrays\n",
    "\t# Iterate over all indexes in x\n",
    "\tit = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\t\n",
    "\t\n",
    "\twhile not it.finished:\n",
    "\t\tix = it.multi_index\t\t\t\t\t#starts from (0, 0) then (0, 1)\n",
    "\t\t\n",
    "\t\tx[ix] += h\t\t\t\t\t\t\t#To calculate [f(xi+h)-f(xi-h)] / 2h\n",
    "\t\trandom.setstate(rndstate)\n",
    "\t\tfxh, _ = f(x)\n",
    "\t\tx[ix] -= 2*h\n",
    "\t\trandom.setstate(rndstate)\n",
    "\t\tfxnh, _ = f(x)\n",
    "\t\tx[ix] += h\n",
    "\t\tnumgrad = (fxh - fxnh) / 2 / h\n",
    "\t\t\t\t\t\t\t\t\t\t\t#To compare gradient calculated by formular and calculus\n",
    "\t\treldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "\t\tif reldiff > 1e-5:\n",
    "\t\t\tprint \"Gradient check failed.\"\n",
    "\t\t\tprint \"First gradient error found at index %s\" % str(ix)\n",
    "\t\t\tprint \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad)\n",
    "\t\t\treturn\n",
    "\t\t\n",
    "\t\tit.iternext()\n",
    "\t\t\n",
    "\tprint \"Gradient check passed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "\n",
    "\tN = x.shape[0]\n",
    "\tx /= np.sqrt(np.sum(x**2, axis=1)).reshape((N,1)) + 1e-30\n",
    "\t\n",
    "\treturn x\n",
    "\n",
    "\n",
    "def test_normalize_rows():\n",
    "\tprint \"Testing normalizeRows...\"\n",
    "\tx = normalizeRows(np.array([[3.0, 4.0],[1, 2]]))\n",
    "\tprint x\n",
    "\tassert (np.amax(np.fabs(x - np.array([[0.6,0.8],[0.4472136,0.89442719]]))) <= 1e-6)\n",
    "\tprint \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"\"\" Softmax cost function for word2vec models \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "\t\"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "\t\n",
    "\tprobabilities = softmax(predicted.dot(outputVectors.T))\t\t\t\n",
    "\tcost = -np.log(probabilities[target])\n",
    "\t\n",
    "\tdelta = probabilities\n",
    "\tdelta[target] -= 1\n",
    "\t\n",
    "\tN = delta.shape[0]\t\t\t\t\t\t\t\t\t\t\t\t#delta.shape = (5,)\n",
    "\tD = predicted.shape[0]\t\t\t\t\t\t\t\t\t\t\t#predicted.shape = (3,)\n",
    "\tgrad = delta.reshape((N, 1)) * predicted.reshape((1, D))\n",
    "\tgradPred = (delta.reshape((1, N)).dot(outputVectors)).flatten()\n",
    "\t\n",
    "\treturn cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"\"\" Skip-gram model in word2vec \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "\tdataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "\t\"\"\" Skip-gram model in word2vec \"\"\"\n",
    "\t\n",
    "\tcurrentI = tokens[currentWord]\t\t\t\t\t\t#the order of this center word in the whole vocabulary\n",
    "\tpredicted = inputVectors[currentI, :]\t\t\t\t#turn this word to vector representation\n",
    "\t\n",
    "\tcost = 0.0\n",
    "\tgradIn = np.zeros(inputVectors.shape)\n",
    "\tgradOut = np.zeros(outputVectors.shape)\n",
    "\tfor cwd in contextWords:\t\t\t\t\t\t\t#contextWords is of 2C length\n",
    "\t\tidx = tokens[cwd]\n",
    "\t\tcc, gp, gg = word2vecCostAndGradient(predicted, idx, outputVectors, dataset)\n",
    "\t\tcost += cc\t\t\t\t\t\t\t\t\t\t#final cost/gradient is the 'sum' of result calculated by each word in context\n",
    "\t\tgradOut += gg\n",
    "\t\tgradIn[currentI, :] += gp\n",
    "\t\n",
    "\treturn cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  word2vec_sgd_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "\tbatchsize = 50\n",
    "\tcost = 0.0\n",
    "\tgrad = np.zeros(wordVectors.shape)   #each element in wordVectors has a gradient\n",
    "\tN = wordVectors.shape[0]\n",
    "\tinputVectors = wordVectors[:N/2, :]\n",
    "\toutputVectors = wordVectors[N/2:, :]\n",
    "\tfor i in xrange(batchsize):\t\t\t\t\t\t\t\t\t#train word2vecModel for 50 times\n",
    "\t\tC1 = random.randint(1, C)\n",
    "\t\tcenterword, context = dataset.getRandomContext(C1)\t\t#randomly choose 1 word, and generate a context of it\n",
    "\t\t\n",
    "\t\tif word2vecModel == skipgram:\n",
    "\t\t\tdenom = 1\n",
    "\t\telse:\n",
    "\t\t\tdenom = 1\n",
    "\t\t\n",
    "\t\tc, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient)\n",
    "\t\tcost += c / batchsize / denom\t\t\t\t\t\t\t#calculate the average\n",
    "\t\tgrad[:N/2, :] += gin / batchsize / denom\n",
    "\t\tgrad[N/2:, :] += gout / batchsize / denom\n",
    "\t\n",
    "\treturn cost, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed\n",
      "\n",
      "=== Results ===\n",
      "(11.166109001533981, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.26947339, -1.36873189,  2.45158957],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.41045956,  0.18834851,  1.43272264],\n",
      "       [ 0.38202831, -0.17530219, -1.33348241],\n",
      "       [ 0.07009355, -0.03216399, -0.24466386],\n",
      "       [ 0.09472154, -0.04346509, -0.33062865],\n",
      "       [-0.13638384,  0.06258276,  0.47605228]]))\n"
     ]
    }
   ],
   "source": [
    "def test_word2vec():\n",
    "    # Interface to the dataset for negative sampling\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] \\\n",
    "           for i in xrange(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "    print \"==== Gradient check for skip-gram ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "\n",
    "    print \"\\n=== Results ===\"\n",
    "    print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_word2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# '''Run word2vec'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE_PARAMS_EVERY = 1000\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import cPickle as pickle\n",
    "\n",
    "def load_saved_params():\n",
    "\t\"\"\" A helper function that loads previously saved parameters and resets iteration start \"\"\"\n",
    "\tst = 0\n",
    "\tfor f in glob.glob(\"saved_params_*.npy\"):\n",
    "\t\titer = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "\t\tif (iter > st):\n",
    "\t\t\tst = iter\n",
    "\t\n",
    "\tif st > 0:\n",
    "\t\twith open(\"saved_params_%d.npy\" % st, \"r\") as f:\n",
    "\t\t\tparams = pickle.load(f)\n",
    "\t\t\tstate = pickle.load(f)\n",
    "\t\treturn st, params, state\n",
    "\telse:\n",
    "\t\treturn st, None, None\n",
    "\n",
    "def save_params(iter, params):\n",
    "\twith open(\"saved_params_%d.npy\" % iter, \"w\") as f:\n",
    "\t\tpickle.dump(params, f)\n",
    "\t\tpickle.dumpy(random.getstate(), f)\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing = None, useSaved = False, PRINT_EVERY = 10):\n",
    "\t\"\"\" Stochastic Gradient Descent \"\"\"\n",
    "\tANNEAL_EVERY = 20000\n",
    "\t\n",
    "\tif useSaved:\n",
    "\t\tstart_iter, oldx, state = load_saved_params()\n",
    "\t\tif start_iter > 0:\n",
    "\t\t\tx0 = oldx;\n",
    "\t\t\tstep *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\t\t\n",
    "\t\tif state:\n",
    "\t\t\trandom.setstate(state)\n",
    "\t\n",
    "\telse:\n",
    "\t\tstart_iter = 0\n",
    "\t\n",
    "\tx = x0\n",
    "\t\n",
    "\tif not postprocessing:\n",
    "\t\tpostprocessing = lambda x: x\n",
    "\t\n",
    "\texpcost = None\n",
    "\t\n",
    "\tfor iter in xrange(start_iter + 1, iterations + 1):\n",
    "\t\t\n",
    "\t\tcost = None\n",
    "\t\t\n",
    "\t\tcost, grad = f(x)\n",
    "\t\tx -= step * grad\n",
    "\t\t\n",
    "\t\tx = postprocessing(x)\n",
    "\t\t\n",
    "\t\tif iter % PRINT_EVERY == 0:\n",
    "\t\t\tif not expcost:\n",
    "\t\t\t\texpcost = cost\n",
    "\t\t\telse:\n",
    "\t\t\t\texpcost = .95 * expcost + .05 * cost\n",
    "\t\t\tprint \"iter %d: %f\" % (iter, expcost)\n",
    "\t\t\t\n",
    "\t\tif iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "\t\t\tsave_params(iter, x)\n",
    "\t\t\n",
    "\t\tif iter % ANNEAL_EVERY == 0:\n",
    "\t\t\tstep *= 0.5\n",
    "\t\t\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cs224.data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "class StanfordSentiment:\n",
    "    def __init__(self, path=None, tablesize = 1000000):\n",
    "        if not path:\n",
    "            path = \"cs224d/datasets/stanfordSentimentTreebank\"\n",
    "\n",
    "        self.path = path\n",
    "        self.tablesize = tablesize\n",
    "\n",
    "    def tokens(self):\n",
    "        if hasattr(self, \"_tokens\") and self._tokens:\n",
    "            return self._tokens\n",
    "\n",
    "        tokens = dict()\n",
    "        tokenfreq = dict()\n",
    "        wordcount = 0\n",
    "        revtokens = []\n",
    "        idx = 0\n",
    "\n",
    "        for sentence in self.sentences():\n",
    "            for w in sentence:\n",
    "                wordcount += 1\n",
    "                if not w in tokens:\n",
    "                    tokens[w] = idx\n",
    "                    revtokens += [w]\n",
    "                    tokenfreq[w] = 1\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    tokenfreq[w] += 1\n",
    "\n",
    "        tokens[\"UNK\"] = idx\n",
    "        revtokens += [\"UNK\"]\n",
    "        tokenfreq[\"UNK\"] = 1\n",
    "        wordcount += 1\n",
    "\n",
    "        self._tokens = tokens\n",
    "        self._tokenfreq = tokenfreq\n",
    "        self._wordcount = wordcount\n",
    "        self._revtokens = revtokens\n",
    "        return self._tokens\n",
    "    \n",
    "    def sentences(self):\n",
    "        if hasattr(self, \"_sentences\") and self._sentences:\n",
    "            return self._sentences\n",
    "\n",
    "        sentences = []\n",
    "        with open(self.path + \"/datasetSentences.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                splitted = line.strip().split()[1:]\n",
    "                # Deal with some peculiar encoding issues with this file\n",
    "                sentences += [[w.lower().decode(\"utf-8\").encode('latin1') for w in splitted]]\n",
    "                \n",
    "        self._sentences = sentences\n",
    "        self._sentlengths = np.array([len(s) for s in sentences])\n",
    "        self._cumsentlen = np.cumsum(self._sentlengths)\n",
    "\n",
    "        return self._sentences\n",
    "\n",
    "    def numSentences(self):\n",
    "        if hasattr(self, \"_numSentences\") and self._numSentences:\n",
    "            return self._numSentences\n",
    "        else:\n",
    "            self._numSentences = len(self.sentences())\n",
    "            return self._numSentences\n",
    "\n",
    "    def allSentences(self):\n",
    "        if hasattr(self, \"_allsentences\") and self._allsentences:\n",
    "            return self._allsentences\n",
    "\n",
    "        sentences = self.sentences()\n",
    "        rejectProb = self.rejectProb()\n",
    "        tokens = self.tokens()\n",
    "        allsentences = [[w for w in s \n",
    "            if 0 >= rejectProb[tokens[w]] or random.random() >= rejectProb[tokens[w]]]\n",
    "            for s in sentences * 30]\n",
    "\n",
    "        allsentences = [s for s in allsentences if len(s) > 1]\n",
    "        \n",
    "        self._allsentences = allsentences\n",
    "        \n",
    "        return self._allsentences\n",
    "\n",
    "    def getRandomContext(self, C=5):\n",
    "        allsent = self.allSentences()\n",
    "        sentID = random.randint(0, len(allsent) - 1)\n",
    "        sent = allsent[sentID]\n",
    "        wordID = random.randint(0, len(sent) - 1)\n",
    "\n",
    "        context = sent[max(0, wordID - C):wordID] \n",
    "        if wordID+1 < len(sent):\n",
    "            context += sent[wordID+1:min(len(sent), wordID + C + 1)]\n",
    "\n",
    "        centerword = sent[wordID]\n",
    "        context = [w for w in context if w != centerword]\n",
    "\n",
    "        if len(context) > 0:\n",
    "            return centerword, context\n",
    "        else:\n",
    "            return self.getRandomContext(C)\n",
    "\n",
    "    def sent_labels(self):\n",
    "        if hasattr(self, \"_sent_labels\") and self._sent_labels:\n",
    "            return self._sent_labels\n",
    "\n",
    "        dictionary = dict()\n",
    "        phrases = 0\n",
    "        with open(self.path + \"/dictionary.txt\", \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                splitted = line.split(\"|\")\n",
    "                dictionary[splitted[0].lower()] = int(splitted[1])\n",
    "                phrases += 1\n",
    "\n",
    "        labels = [0.0] * phrases\n",
    "        with open(self.path + \"/sentiment_labels.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                splitted = line.split(\"|\")\n",
    "                labels[int(splitted[0])] = float(splitted[1])\n",
    "\n",
    "        sent_labels = [0.0] * self.numSentences()\n",
    "        sentences = self.sentences()\n",
    "        for i in xrange(self.numSentences()):\n",
    "            sentence = sentences[i]\n",
    "            full_sent = \" \".join(sentence).replace('-lrb-', '(').replace('-rrb-', ')')\n",
    "            sent_labels[i] = labels[dictionary[full_sent]]\n",
    "            \n",
    "        self._sent_labels = sent_labels\n",
    "        return self._sent_labels\n",
    "\n",
    "    def dataset_split(self):\n",
    "        if hasattr(self, \"_split\") and self._split:\n",
    "            return self._split\n",
    "\n",
    "        split = [[] for i in xrange(3)]\n",
    "        with open(self.path + \"/datasetSplit.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                splitted = line.strip().split(\",\")\n",
    "                split[int(splitted[1]) - 1] += [int(splitted[0]) - 1]\n",
    "\n",
    "        self._split = split\n",
    "        return self._split\n",
    "\n",
    "    def getRandomTrainSentence(self):\n",
    "        split = self.dataset_split()\n",
    "        sentId = split[0][random.randint(0, len(split[0]) - 1)]\n",
    "        return self.sentences()[sentId], self.categorify(self.sent_labels()[sentId])\n",
    "\n",
    "    def categorify(self, label):\n",
    "        if label <= 0.2:\n",
    "            return 0\n",
    "        elif label <= 0.4:\n",
    "            return 1\n",
    "        elif label <= 0.6:\n",
    "            return 2\n",
    "        elif label <= 0.8:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "\n",
    "    def getDevSentences(self):\n",
    "        return self.getSplitSentences(2)\n",
    "\n",
    "    def getTestSentences(self):\n",
    "        return self.getSplitSentences(1)\n",
    "\n",
    "    def getTrainSentences(self):\n",
    "        return self.getSplitSentences(0)\n",
    "\n",
    "    def getSplitSentences(self, split=0):\n",
    "        ds_split = self.dataset_split()\n",
    "        return [(self.sentences()[i], self.categorify(self.sent_labels()[i])) for i in ds_split[split]]\n",
    "\n",
    "    def sampleTable(self):\n",
    "        if hasattr(self, '_sampleTable') and self._sampleTable is not None:\n",
    "            return self._sampleTable\n",
    "\n",
    "        nTokens = len(self.tokens())\n",
    "        samplingFreq = np.zeros((nTokens,))\n",
    "        self.allSentences()\n",
    "        i = 0\n",
    "        for w in xrange(nTokens):\n",
    "            w = self._revtokens[i]\n",
    "            if w in self._tokenfreq:\n",
    "                freq = 1.0 * self._tokenfreq[w]\n",
    "                # Reweigh\n",
    "                freq = freq ** 0.75\n",
    "            else:\n",
    "                freq = 0.0\n",
    "            samplingFreq[i] = freq\n",
    "            i += 1\n",
    "\n",
    "        samplingFreq /= np.sum(samplingFreq)\n",
    "        samplingFreq = np.cumsum(samplingFreq) * self.tablesize\n",
    "\n",
    "        self._sampleTable = [0] * self.tablesize\n",
    "\n",
    "        j = 0\n",
    "        for i in xrange(self.tablesize):\n",
    "            while i > samplingFreq[j]:\n",
    "                j += 1\n",
    "            self._sampleTable[i] = j\n",
    "\n",
    "        return self._sampleTable\n",
    "\n",
    "    def rejectProb(self):\n",
    "        if hasattr(self, '_rejectProb') and self._rejectProb is not None:\n",
    "            return self._rejectProb\n",
    "\n",
    "        threshold = 1e-5 * self._wordcount\n",
    "\n",
    "        nTokens = len(self.tokens())\n",
    "        rejectProb = np.zeros((nTokens,))\n",
    "        for i in xrange(nTokens):\n",
    "            w = self._revtokens[i]\n",
    "            freq = 1.0 * self._tokenfreq[w]\n",
    "            # Reweigh\n",
    "            rejectProb[i] = max(0, 1 - np.sqrt(threshold / freq))\n",
    "\n",
    "        self._rejectProb = rejectProb\n",
    "        return self._rejectProb\n",
    "\n",
    "    def sampleTokenIdx(self):\n",
    "        return self.sampleTable()[random.randint(0, self.tablesize - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "# from cs224.data_utils import *    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from c5_word2vec import *\n",
    "# from c6_sgd import *\n",
    "\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "wordVectors = np.concatenate(((np.random.rand(nWords, dimVectors) - .5) / \\\n",
    "\tdimVectors, np.zeros((nWords, dimVectors))), axis=0)\n",
    "\t\n",
    "wordVectors0 = sgd(\n",
    "\tlambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
    "\t\tnegSamplingCostAndGradient),\n",
    "\t\twordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "\n",
    "print \"sanity check: cost at convergence should be around or below 10\"\n",
    "\n",
    "wordVectors = (wordVectors0[:nWords, :] + wordVectors0[nWords:, :])\n",
    "\n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "\n",
    "wordVectors = (wordVectors0[:nWords, :] + wordVectors0[nWords:, :])\n",
    "\n",
    "visualizeWords = [\"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\", \n",
    "\t\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "\t\"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\", \n",
    "\t\"annoying\"]\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U, S, V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:, 0:2])\n",
    "\n",
    "for i in xrange(len(visualizeWords)):\n",
    "\tplt.text(coord[i, 0], coord[i, 1], visualizeWords[i],\n",
    "\t\tbbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:, 0]), np.max(coord[:, 0])))\n",
    "plt.ylim((np.min(coord[:, 1]), np.max(coord[:, 1])))\n",
    "\n",
    "plt.savefig('q3_word_vectors.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
